{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  # tf.keras\n",
    "\n",
    "#set seed for reproducibility of results\n",
    "np.random.seed(42) \n",
    "tf.random.set_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mela = pd.read_excel('Melanoma1517_aggiornato_12042022_sub.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mela.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in mela.columns:\n",
    "    print('\\n-----{}-----'.format(c))\n",
    "    print(mela[c].value_counts())\n",
    "    print('Missin values ', np.sum(mela[c].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mela['eta_diagn']))\n",
    "print(np.median(mela['eta_diagn']))\n",
    "print(np.min(mela['eta_diagn']))\n",
    "print(np.max(mela['eta_diagn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mela['Sopravvivenza_3_anni']))\n",
    "print(np.median(mela['Sopravvivenza_3_anni']))\n",
    "print(np.min(mela['Sopravvivenza_3_anni']))\n",
    "print(np.max(mela['Sopravvivenza_3_anni']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmean(mela['mitosi']))\n",
    "print(np.nanmedian(mela['mitosi']))\n",
    "print(np.min(mela['mitosi']))\n",
    "print(np.max(mela['mitosi']))\n",
    "print(np.sum(mela['mitosi'].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmean(mela['bls_diametro_massimo_metastasi_m']))\n",
    "print(np.nanmedian(mela['bls_diametro_massimo_metastasi_m']))\n",
    "print(np.min(mela['bls_diametro_massimo_metastasi_m']))\n",
    "print(np.max(mela['bls_diametro_massimo_metastasi_m']))\n",
    "print(np.sum(mela['bls_diametro_massimo_metastasi_m'].isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# simplify stage T classes and replace missing with 'pT0/pTX/DM' class\n",
    "\n",
    "mela['STADIOT_VER8'].replace('pT1a', 'pT1', inplace = True)\n",
    "mela['STADIOT_VER8'].replace('pT1b', 'pT1', inplace = True)\n",
    "mela['STADIOT_VER8'].replace('pT2a', 'pT2', inplace = True)\n",
    "mela['STADIOT_VER8'].replace('pT2b', 'pT2', inplace = True)\n",
    "mela['STADIOT_VER8'].replace('pT3a', 'pT3', inplace = True)\n",
    "mela['STADIOT_VER8'].replace('pT3b', 'pT3', inplace = True)\n",
    "mela['STADIOT_VER8'].replace('pT4a', 'pT4', inplace = True)\n",
    "mela['STADIOT_VER8'].replace('pT4b', 'pT4', inplace = True)\n",
    "\n",
    "#mela['STADIOT_VER8'].replace('pTX', 'pT0/pTX/DM', inplace = True)\n",
    "\n",
    "mela['STADIOT_VER8'].replace(np.nan, 'pT0/DM', inplace = True)\n",
    "mela['STADIOT_VER8'].replace('pT0', 'pT0/DM', inplace = True)\n",
    "\n",
    "mela['STADIOT_VER8'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mela['STADIOT_VER8'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing stage N with 'N0/DM' class\n",
    "\n",
    "mela['STADION_VER8'].replace(np.nan, 'NX/DM', inplace = True)\n",
    "#mela['STADION_VER8'].replace('N0', 'N0/DM', inplace = True)\n",
    "mela['STADION_VER8'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mela['STADION_VER8'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing stage M with 'M0/DM' class\n",
    "\n",
    "mela['STADIOM_VER8'].replace(np.nan, 'M0/DM', inplace = True)\n",
    "mela['STADIOM_VER8'].replace('M0', 'M0/DM', inplace = True)\n",
    "mela['STADIOM_VER8'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mela['STADIOM_VER8'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert hystology with original class names (please refere to 'Tracciato record registro Melanoma')\n",
    "\n",
    "mela['ISTOGENETICA'].value_counts() # ci sono solo 0,1,2,3,4,5,6,10\n",
    "\n",
    "mela['ISTOGENETICA'].replace(0, 'Malignant melanoma', inplace = True)\n",
    "mela['ISTOGENETICA'].replace(1, 'Superficial spreading melanoma', inplace = True)\n",
    "mela['ISTOGENETICA'].replace(2, 'Nodular melanoma', inplace = True)\n",
    "mela['ISTOGENETICA'].replace(3, 'Lentigo maligna', inplace = True)\n",
    "mela['ISTOGENETICA'].replace(4, 'Acral-lentiginous melanoma', inplace = True)\n",
    "mela['ISTOGENETICA'].replace(5, 'Desmoplastic melanoma', inplace = True)\n",
    "mela['ISTOGENETICA'].replace(6, 'Melanoma arising from blue naevus', inplace = True)\n",
    "mela['ISTOGENETICA'].replace(10, 'Spitzoid melanoma', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 'DM' with np.nan to fill with imputation techniques\n",
    "\n",
    "mela['ulcerazione'].replace('DM',np.nan, inplace = True)\n",
    "mela['regressione'].replace('DM',np.nan, inplace = True)\n",
    "mela['tipo_crescita'].replace('DM',np.nan, inplace = True)\n",
    "mela['til'].replace('DM',np.nan, inplace = True)\n",
    "mela['morfologia_2'].replace('DM',np.nan, inplace = True)\n",
    "mela['sede_2'].replace('DM',np.nan, inplace = True)\n",
    "mela['breslow_new'].replace('DM',np.nan, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing costs with 0 (assuming expense not performed)\n",
    "\n",
    "mela['COSTO_MELA_app_spsY1'].replace(np.nan, 0, inplace = True)\n",
    "mela['COSTO_MELA_app_spsY2'].replace(np.nan, 0, inplace = True)\n",
    "mela['COSTO_SPS_MELA'].replace(np.nan, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary variable for positive SLNB (positive if number >= 1)\n",
    "\n",
    "mela['SLNB Positive'] = 1*(mela.bls_linfo_positivi >= 1) # in this way the variable is already numerically encoded (1 when positive, 0 otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing diameters with 0 (assuming exam not performed)\n",
    "\n",
    "mela['bls_diametro_massimo_metastasi_m'].replace(np.nan, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total count positive lymph nodes (SLNB + lymphadenectomy)\n",
    "\n",
    "mela['Positive Lymph Nodes'] = mela['bls_linfo_positivi'].replace(np.nan, 0) + mela['la_linfo_positivi'].replace(np.nan, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mela['la_linfo_positivi'].isna() & mela['bls_linfo_positivi'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mela['la_linfo_positivi'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmean(mela['Positive Lymph Nodes']))\n",
    "print(np.nanmedian(mela['Positive Lymph Nodes']))\n",
    "print(np.min(mela['Positive Lymph Nodes']))\n",
    "print(np.max(mela['Positive Lymph Nodes']))\n",
    "print(np.sum(mela['Positive Lymph Nodes'].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmean(mela.loc[~(mela['la_linfo_positivi'].isna() & mela['bls_linfo_positivi'].isna()),'Positive Lymph Nodes']))\n",
    "print(np.nanmedian(mela.loc[~(mela['la_linfo_positivi'].isna() & mela['bls_linfo_positivi'].isna()),'Positive Lymph Nodes']))\n",
    "print(np.min(mela.loc[~(mela['la_linfo_positivi'].isna() & mela['bls_linfo_positivi'].isna()), 'Positive Lymph Nodes']))\n",
    "print(np.max(mela.loc[~(mela['la_linfo_positivi'].isna() & mela['bls_linfo_positivi'].isna()), 'Positive Lymph Nodes']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually encode bleslow as it still contains nans\n",
    "\n",
    "mela['breslow_new'].replace('DM',np.nan, inplace = True)\n",
    "mela['breslow_new'].replace('G-F041',1.0, inplace = True)\n",
    "mela['breslow_new'].replace('G-F042',2.0, inplace = True)\n",
    "mela['breslow_new'].replace('G-F043',3.0, inplace = True)\n",
    "mela['breslow_new'].replace('G-F044',4.0, inplace = True)\n",
    "\n",
    "print(mela['breslow_new'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mela = mela.join(pd.get_dummies(mela['ISTOGENETICA'], prefix='istogenetica', drop_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mela = mela.join(pd.get_dummies(mela['sesso'], prefix='sesso', drop_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mela = mela.join(pd.get_dummies(mela['STADIOT_VER8'], prefix='stadioT', drop_first=False))\n",
    "mela.drop('stadioT_pT0/DM', axis = 1, inplace = True) # manually drop class 'pT0/DM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mela = mela.join(pd.get_dummies(mela['STADION_VER8'], prefix='stadioN', drop_first=False))\n",
    "mela.drop('stadioN_NX/DM', axis = 1, inplace = True) # manually drop class 'NX/DM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mela = mela.join(pd.get_dummies(mela['STADIOM_VER8'], prefix='stadioM', drop_first=False))\n",
    "mela.drop('stadioM_M0/DM', axis = 1, inplace = True) # manually drop class 'M0/DM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually encode ulceration as it still contains nans\n",
    "\n",
    "mela['ulcerazione_Presente'] = mela['ulcerazione'] \n",
    "\n",
    "mela['ulcerazione_Presente'].replace('Presente',1, inplace = True)\n",
    "mela['ulcerazione_Presente'].replace('Assente',0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually encode regression as it still contains nans\n",
    "\n",
    "mela['regressione_Presente'] = mela['regressione'] \n",
    "\n",
    "mela['regressione_Presente'].replace('Presente',1, inplace = True)\n",
    "mela['regressione_Presente'].replace('Assente',0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually encode growtn type as it still contains nans\n",
    "\n",
    "mela['tipo_crescita_Verticale'] = mela['tipo_crescita'] \n",
    "\n",
    "mela['tipo_crescita_Verticale'].replace('Verticale',1, inplace = True)\n",
    "mela['tipo_crescita_Verticale'].replace('Orizzontale',0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually encode TILs as it still contains nans\n",
    "\n",
    "mela['til_Presente'] = mela['til'] \n",
    "\n",
    "mela['til_Presente'].replace('Presente',1, inplace = True)\n",
    "mela['til_Presente'].replace('Assente',0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset of complete variables (exclude survival and costs outcomes, marital status, educational level and lymphnodes data)\n",
    "\n",
    "X = mela.loc[:,['sesso_M', 'eta_diagn',\n",
    "                'stadioT_pT1', 'stadioT_pT2', 'stadioT_pT3', 'stadioT_pT4', 'stadioT_pTX', 'stadioN_N0', 'stadioN_N1a', 'stadioN_N1b', 'stadioN_N1c', 'stadioN_N2a', 'stadioN_N2b', 'stadioN_N2c', 'stadioN_N3', 'stadioN_N3c', 'stadioM_M1', #'stadioM', 'stadioN', 'stadioT', \n",
    "                'istogenetica_Desmoplastic melanoma', 'istogenetica_Lentigo maligna', 'istogenetica_Malignant melanoma', 'istogenetica_Melanoma arising from blue naevus', 'istogenetica_Nodular melanoma', 'istogenetica_Spitzoid melanoma','istogenetica_Superficial spreading melanoma', \n",
    "                ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute mitotic count with linear regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train = X.loc[~mela.mitosi.isna()]\n",
    "y_train = mela.loc[~mela.mitosi.isna(), 'mitosi']\n",
    "X_test = X.loc[mela.mitosi.isna()]\n",
    "\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "y_pred = y_pred * (y_pred>=0)\n",
    "\n",
    "mela.loc[mela.mitosi.isna(), 'mitosi'] = y_pred\n",
    "mela.mitosi.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute ulceration with logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train = X.loc[~mela.ulcerazione_Presente.isna()]\n",
    "y_train = mela.loc[~mela.ulcerazione_Presente.isna(), 'ulcerazione_Presente']\n",
    "X_test = X.loc[mela.ulcerazione_Presente.isna()]\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter= 500).fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_train, y_train)) # better than majority class \n",
    "print(mela.ulcerazione_Presente.value_counts()/(np.sum(mela.ulcerazione_Presente.isna()==False)))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "mela.loc[mela.ulcerazione_Presente.isna(), 'ulcerazione_Presente'] = y_pred\n",
    "mela.ulcerazione_Presente.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute tumor regression with logistic regression\n",
    "\n",
    "X_train = X.loc[~mela.regressione_Presente.isna()]\n",
    "y_train = mela.loc[~mela.regressione_Presente.isna(), 'regressione_Presente']\n",
    "X_test = X.loc[mela.regressione_Presente.isna()]\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter= 500).fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_train, y_train)) # better than majority class \n",
    "print(mela.regressione_Presente.value_counts()/(np.sum(mela.regressione_Presente.isna()==False)))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "mela.loc[mela.regressione_Presente.isna(), 'regressione_Presente'] = y_pred\n",
    "mela.regressione_Presente.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute growth type with logistic regression\n",
    "\n",
    "X_train = X.loc[~mela.tipo_crescita_Verticale.isna()]\n",
    "y_train = mela.loc[~mela.tipo_crescita_Verticale.isna(), 'tipo_crescita_Verticale']\n",
    "X_test = X.loc[mela.tipo_crescita_Verticale.isna()]\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter= 500).fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_train, y_train)) # better than majority class\n",
    "print(mela.tipo_crescita_Verticale.value_counts()/(np.sum(mela.tipo_crescita_Verticale.isna()==False)))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "mela.loc[mela.tipo_crescita_Verticale.isna(), 'tipo_crescita_Verticale'] = y_pred\n",
    "mela.tipo_crescita_Verticale.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute TILs with logistic regression\n",
    "\n",
    "X_train = X.loc[~mela.til_Presente.isna()]\n",
    "y_train = mela.loc[~mela.til_Presente.isna(), 'til_Presente']\n",
    "X_test = X.loc[mela.til_Presente.isna()]\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter= 500).fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_train, y_train)) # better than majority class\n",
    "print(mela.til_Presente.value_counts()/(np.sum(mela.til_Presente.isna()==False)))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "mela.loc[mela.til_Presente.isna(), 'til_Presente'] = y_pred\n",
    "mela.til_Presente.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute morfology with multiclass logistic regression\n",
    "\n",
    "X_train = X.loc[~mela.morfologia_2.isna()]\n",
    "y_train = mela.loc[~mela.morfologia_2.isna(), 'morfologia_2']\n",
    "X_test = X.loc[mela.morfologia_2.isna()]\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter= 1000).fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train)) # better than majority class\n",
    "print(mela.morfologia_2.value_counts()/(np.sum(mela.morfologia_2.isna()==False)))\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "mela.loc[mela.morfologia_2.isna(), 'morfologia_2'] = y_pred\n",
    "mela.morfologia_2.isna().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute tumor site with multiclass logistic regression\n",
    "\n",
    "X_train = X.loc[~mela.sede_2.isna()]\n",
    "y_train = mela.loc[~mela.sede_2.isna(), 'sede_2']\n",
    "X_test = X.loc[mela.sede_2.isna() ]\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter= 1000).fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train)) # better than majority class\n",
    "print(mela.sede_2.value_counts()/(np.sum(mela.sede_2.isna()==False)))\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "mela.loc[mela.sede_2.isna(), 'sede_2'] = y_pred\n",
    "mela.sede_2.isna().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute breslow with multiclass logistic regression (even if continuous var.. it seems to work better)\n",
    "\n",
    "X_train = X.loc[~mela.breslow_new.isna()]\n",
    "y_train = mela.loc[~mela.breslow_new.isna(), 'breslow_new']\n",
    "X_test = X.loc[mela.breslow_new.isna()]\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter= 1000).fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train)) # better than majority class\n",
    "print(mela.breslow_new.value_counts()/(np.sum(mela.breslow_new.isna()==False)))\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "mela.loc[mela.breslow_new.isna(), 'breslow_new'] = y_pred\n",
    "mela.breslow_new.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it is possible to encode morphology and tumor site \n",
    "\n",
    "mela = mela.join(pd.get_dummies(mela['morfologia_2'], prefix='morfologia', drop_first=False))\n",
    "mela.drop('morfologia_Altro', axis = 1, inplace = True) # scelgo a mano di droppare 'Altro'\n",
    "\n",
    "mela = mela.join(pd.get_dummies(mela['sede_2'], prefix='sede', drop_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create breslow categorical version with original class names\n",
    "\n",
    "mela['breslow_cat'] = mela['breslow_new'] \n",
    "mela['breslow_cat'].replace(1,'<0.75 mm', inplace = True)\n",
    "mela['breslow_cat'].replace(2,'0.76–1.50 mm', inplace = True)\n",
    "mela['breslow_cat'].replace(3,'1.51–3.99 mm', inplace = True)\n",
    "mela['breslow_cat'].replace(4,'≥4 mm', inplace = True)\n",
    "\n",
    "mela['breslow_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mela.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataset to use for ML models\n",
    "\n",
    "\n",
    "mela_cleaned = mela.loc[:,['sesso_M', 'eta_diagn', \n",
    "                   'breslow_new', 'mitosi',  \n",
    "                   'STADIOT_VER8', 'STADION_VER8', 'STADIOM_VER8', 'ISTOGENETICA', 'sede_2', 'breslow_cat', # not encoded version\n",
    "                   'istogenetica_Desmoplastic melanoma', 'istogenetica_Lentigo maligna', 'istogenetica_Malignant melanoma', 'istogenetica_Melanoma arising from blue naevus', 'istogenetica_Nodular melanoma', 'istogenetica_Spitzoid melanoma', 'istogenetica_Superficial spreading melanoma',  \n",
    "                   'stadioT_pT1', 'stadioT_pT2', 'stadioT_pT3', 'stadioT_pT4', 'stadioT_pTX', \n",
    "                   'stadioN_N0', 'stadioN_N1a', 'stadioN_N1b', 'stadioN_N1c', 'stadioN_N2a', 'stadioN_N2b', 'stadioN_N2c', 'stadioN_N3', 'stadioN_N3c', 'stadioM_M1',\n",
    "                   'ulcerazione_Presente', 'regressione_Presente', 'tipo_crescita_Verticale', 'til_Presente', \n",
    "                    #'morfologia_Acrale', 'morfologia_Cellule Epitelioidi', 'morfologia_Diffusione Superficiale', 'morfologia_Lentigo Maligna', 'morfologia_NAS', 'morfologia_Nodulare',\n",
    "                   'sede_Arti superiori', 'sede_Capo', 'sede_Estremita', 'sede_Tronco',\n",
    "                   'SLNB Positive', 'bls_diametro_massimo_metastasi_m',  'Positive Lymph Nodes',\n",
    "                   'DECESSO_3_anni', 'Sopravvivenza_3_anni',\n",
    "                   'COSTO_COMPLESSIVO_MELA', 'COSTO_SDO_MELA', 'COSTO_SPS_MELA', 'COSTO_F3_MELA', 'COSTO_CINECA_MELA'\n",
    "            ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns in english\n",
    "\n",
    "mela_cleaned.rename(columns={'sesso_M' : 'Male', 'eta_diagn': 'age',\n",
    "                'breslow_new':'Breslow thickness group', 'breslow_cat' : 'Breslow thickness',\n",
    "                'STADIOT_VER8' : 'T stage', 'STADION_VER8' : 'N stage', 'STADIOM_VER8' : 'M stage', 'ISTOGENETICA' : 'Histology', 'sede_2' : 'Tumor site',\n",
    "                'stadioT_pT1' : 'T1 stage', 'stadioT_pT2': 'T2 stage', 'stadioT_pT3': 'T3 stage', 'stadioT_pT4': 'T4 stage', 'stadioT_pTX': 'TX stage', \n",
    "                'stadioN_N0': 'N0 stage', 'stadioN_N1a': 'N1a stage', 'stadioN_N1b': 'N1b stage', 'stadioN_N1c': 'N1c stage', 'stadioN_N2a': 'N2a stage', 'stadioN_N2b': 'N2b stage', 'stadioN_N2c': 'N2c stage', 'stadioN_N3': 'N3 stage', 'stadioN_N3c': 'N3c stage',\n",
    "                'stadioM_M1' : 'M1 stage', \n",
    "                'istogenetica_Desmoplastic melanoma' : 'Histology Desmoplastic melanoma', 'istogenetica_Lentigo maligna' : 'Histology Lentigo maligna', 'istogenetica_Malignant melanoma' : 'Histology Malignant melanoma', 'istogenetica_Melanoma arising from blue naevus' : 'Histology Melanoma arising from blue naevus', 'istogenetica_Nodular melanoma' :  'Histology Nodular melanoma', 'istogenetica_Spitzoid melanoma' : 'Histology Spitzoid melanoma','istogenetica_Superficial spreading melanoma' : 'Histology Superficial spreading melanoma', \n",
    "                'mitosi' : 'Mitotic count', 'ulcerazione_Presente' : 'Ulceration Present', 'regressione_Presente' : 'Tumor regression Present', 'tipo_crescita_Verticale' : 'Growth pattern Vertical', 'til_Presente' : 'TIL Present',\n",
    "                \n",
    "                'sede_Arti superiori': 'Tumor site Upper limb', 'sede_Capo':'Tumor site Head','sede_Estremita':'Tumor site Hands/Feet','sede_Tronco': 'Tumor site Trunk',\n",
    "                'bls_diametro_massimo_metastasi_m' : 'SLNB max diameter'}, inplace = True )\n",
    "\n",
    "\n",
    "\n",
    "                 \n",
    "                     \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export complete and cleaned dataset\n",
    "\n",
    "mela_cleaned.to_excel('Melanoma1517_sub_cleaned.xlsx', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "\n",
    "mela_cleaned = pd.read_excel('Melanoma1517_sub_cleaned.xlsx') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only numerical (encoded)\n",
    "\n",
    "X = mela_cleaned.loc[:,['Male', 'age', 'Breslow thickness group', 'Mitotic count',\n",
    "       'Histology Desmoplastic melanoma', 'Histology Lentigo maligna',\n",
    "       'Histology Malignant melanoma',\n",
    "       'Histology Melanoma arising from blue naevus',\n",
    "       'Histology Nodular melanoma', 'Histology Spitzoid melanoma',\n",
    "       'Histology Superficial spreading melanoma', 'T1 stage', 'T2 stage',\n",
    "       'T3 stage', 'T4 stage', 'TX stage', 'N0 stage', 'N1a stage', 'N1b stage', 'N1c stage',\n",
    "       'N2a stage', 'N2b stage', 'N2c stage', 'N3 stage', 'N3c stage',\n",
    "       'M1 stage', 'Ulceration Present', 'Tumor regression Present',\n",
    "       'Growth pattern Vertical', 'TIL Present', 'Tumor site Upper limb',\n",
    "       'Tumor site Head', 'Tumor site Hands/Feet', 'Tumor site Trunk',\n",
    "       'SLNB Positive', 'SLNB max diameter', 'Positive Lymph Nodes'  \n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(X.corr(), fignum=f.number)\n",
    "plt.xticks(range(X.shape[1]), X.columns, fontsize=14, rotation=90)\n",
    "plt.yticks(range(X.shape[1]), X.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define two alternative set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop correlate (validating...)\n",
    "\n",
    "X1 = X.drop(['Breslow thickness group', 'SLNB Positive', 'SLNB max diameter', 'Positive Lymph Nodes'], \n",
    "       axis = 1)\n",
    "\n",
    "X2 = X.drop(['T1 stage', 'T2 stage', 'T3 stage', 'T4 stage', 'TX stage', 'N0 stage', 'N1a stage', 'N1b stage', 'N1c stage', 'N2a stage', 'N2b stage', 'N2c stage', 'N3 stage', 'N3c stage'], \n",
    "       axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = StandardScaler().fit_transform(X) #features should be scaled!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "principalComponents = pca.fit_transform(X_scaled)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "\n",
    "finalDf = pd.concat([principalDf, mela_cleaned[['DECESSO_3_anni']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot observations 2 pcs\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "\n",
    "ax.set_title('2 components PCA', fontsize = 20)\n",
    "targets = [0,1]\n",
    "colors = ['gold', 'cornflowerblue']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['DECESSO_3_anni'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'PC1']\n",
    "               , finalDf.loc[indicesToKeep, 'PC2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot features with higher coeff (in absolute terms) in pc1 and 2\n",
    "\n",
    "y = mela_cleaned[['DECESSO_3_anni']]\n",
    "\n",
    "c_map = {0: 'gold', 1: 'cornflowerblue'}\n",
    "def myplot(score,coeff,labels=None, coeff_tr = 0.05):\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    for i in range(len(xs)):\n",
    "        plt.scatter(xs[i] * scalex,ys[i] * scaley, alpha = 0.5, color = c_map[y.values[i,0]])\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'k',alpha = 0.5)\n",
    "        \n",
    "        lenght = np.sqrt(coeff[i,0]**2 + coeff[i,1]**2)\n",
    "        \n",
    "        if labels is None:\n",
    "            if lenght>coeff_tr:\n",
    "                plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'k', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            if lenght>coeff_tr:\n",
    "                plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'k', ha = 'center', va = 'center')\n",
    "    plt.xlim(-0.75,0.75)\n",
    "    plt.ylim(-0.75,0.75)\n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))\n",
    "    plt.grid()\n",
    "    \n",
    "\n",
    "\n",
    "l = list(X.columns)\n",
    "\n",
    "#Call the function. Use only the 2 PCs.\n",
    "myplot(principalComponents[:,0:2],np.transpose(pca.components_[0:2, :]),l, 0.30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "principalComponents = pca.fit_transform(X_scaled)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "\n",
    "\n",
    "finalDf = pd.concat([principalDf, mela_cleaned[['DECESSO_3_anni']]], axis = 1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "targets = [0,1]\n",
    "colors = ['gold', 'cornflowerblue']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['DECESSO_3_anni'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               ,finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               ,finalDf.loc[indicesToKeep, 'principal component 3']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha = 0.5)\n",
    "ax.legend(targets)\n",
    "ax.set_xlabel('PC1', fontsize = 15)\n",
    "ax.set_ylabel('PC2', fontsize = 15)\n",
    "ax.set_zlabel('PC3', fontsize = 15)\n",
    "\n",
    "#ax.set_title('3 components PCA', fontsize = 20)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_ # <30% :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.15825628+ 0.06533917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.15825628+ 0.06533917+ 0.05000735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum number of components for 80% explained variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use n components to have 80% variability explained\n",
    "pca = PCA(.80)\n",
    "X3_array = pca.fit_transform(X_scaled)\n",
    "X3 = pd.DataFrame(X3_array) # as df (required by functions below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3.shape #pca.n_components_ 20 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape # compared to 37 original num. of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape # compared to 33 features in subset 1 (no breslow and lymphnodes info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape # compared to 23 features in subset 2 (no T, N values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "### Task: Predict Survival/Death in 3 years from diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define target variable and split train and test records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target/label\n",
    "y = mela_cleaned['DECESSO_3_anni']\n",
    "\n",
    "# split train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, #X1, X2, X3\n",
    "                                                    y, \n",
    "                                                    test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mela_cleaned['DECESSO_3_anni'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utils functions for sklearn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clf_gridsearchcv_scores(clf, parameters, X_train, y_train, name):\n",
    "\n",
    "    # perform cross validation\n",
    "    cv = 5 #10\n",
    "    \n",
    "    gs = GridSearchCV(clf, parameters, \n",
    "                       scoring = ('balanced_accuracy', 'accuracy','f1', 'precision', 'recall'), # measures to evaluate\n",
    "                       refit = 'balanced_accuracy', # measure with which decide best params\n",
    "                       cv = cv) # n fold in statified CV\n",
    "    \n",
    "    gs.fit(X_train, y_train) # fit gridsearch\n",
    "    \n",
    "    index_best = gs.cv_results_['params'].index(gs.best_params_) # position of best params\n",
    "\n",
    "    # mean scores in cv with best params\n",
    "\n",
    "    gs.cv_results_['mean_test_balanced_accuracy'][index_best]\n",
    "    gs.cv_results_['std_test_balanced_accuracy'][index_best]\n",
    "    gs.cv_results_['mean_test_accuracy'][index_best]\n",
    "    gs.cv_results_['std_test_accuracy'][index_best]\n",
    "    gs.cv_results_['mean_test_f1'][index_best]\n",
    "    gs.cv_results_['std_test_f1'][index_best]\n",
    "    gs.cv_results_['mean_test_precision'][index_best]\n",
    "    gs.cv_results_['std_test_precision'][index_best]\n",
    "    gs.cv_results_['mean_test_recall'][index_best]\n",
    "    gs.cv_results_['std_test_recall'][index_best]\n",
    "    \n",
    "    print('-------- CV MEAN SCORES (k = '+str(cv)+') ' + name + ' --------')\n",
    "    print(\"%0.3f accuracy with a standard deviation of %0.3f\" % (gs.cv_results_['mean_test_balanced_accuracy'][index_best], gs.cv_results_['std_test_balanced_accuracy'][index_best]))\n",
    "    print(\"%0.3f balanced accuracy with a standard deviation of %0.3f\" % (gs.cv_results_['mean_test_balanced_accuracy'][index_best], gs.cv_results_['std_test_balanced_accuracy'][index_best]))  \n",
    "    print(\"%0.3f precision with a standard deviation of %0.3f\" % (gs.cv_results_['mean_test_precision'][index_best], gs.cv_results_['std_test_precision'][index_best]))\n",
    "    print(\"%0.3f recall with a standard deviation of %0.3f\" % (gs.cv_results_['mean_test_recall'][index_best],gs.cv_results_['std_test_recall'][index_best]))\n",
    "    print(\"%0.3f f1 score with a standard deviation of %0.3f\" % (gs.cv_results_['mean_test_f1'][index_best],gs.cv_results_['std_test_f1'][index_best]))\n",
    "    \n",
    "    return(gs.best_params_)\n",
    "\n",
    "def clf_fit_print_test_scores(clf, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # final fit and testing\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    # test model \n",
    "    print('-------------------- TRAIN SCORES -------------------')\n",
    "    predictions = clf.predict(X_train)\n",
    "    #print(classification_report(predictions, y_train))\n",
    "    print(\"%0.3f accuracy\" % (accuracy_score(predictions, y_train)))\n",
    "    print(\"%0.3f balanced\" % (balanced_accuracy_score(predictions, y_train)))  \n",
    "    print(\"%0.3f precision\" % (precision_score(predictions, y_train)))\n",
    "    print(\"%0.3f recall\" % (recall_score(predictions, y_train)))\n",
    "    print(\"%0.3f f1 score\" % (f1_score(predictions, y_train)))\n",
    "\n",
    "\n",
    "\n",
    "    print('-------------------- TEST SCORES ---------------------')\n",
    "    predictions = clf.predict(X_test)\n",
    "    #print(classification_report(predictions, y_test))\n",
    "    print(\"%0.3f accuracy\" % (accuracy_score(predictions, y_test)))\n",
    "    print(\"%0.3f balanced\" % (balanced_accuracy_score(predictions, y_test)))  \n",
    "    print(\"%0.3f precision\" % (precision_score(predictions, y_test)))\n",
    "    print(\"%0.3f recall\" % (recall_score(predictions, y_test)))\n",
    "    print(\"%0.3f f1 score\" % (f1_score(predictions, y_test)))\n",
    "\n",
    "    return clf\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Good baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch cross validation for hyperparameter tuning\n",
    "\n",
    "parameters = {'max_iter':(300, 1000), 'C':[10e30, 10e5]}\n",
    "clf = LogisticRegression(random_state=0, class_weight = 'balanced')\n",
    "\n",
    "best_params = print_clf_gridsearchcv_scores(clf, parameters, X_train, y_train, 'LR')\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clf with best params\n",
    "\n",
    "clf = LogisticRegression(random_state=0, class_weight = 'balanced', **best_params).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final fit and evaluate model\n",
    "clf = clf_fit_print_test_scores(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to plot regression summary output (similar to R lm)\n",
    "def logit_pvalue(model, x):\n",
    "    \"\"\" Calculate z-scores for scikit-learn LogisticRegression.\n",
    "    parameters:\n",
    "        model: fitted sklearn.linear_model.LogisticRegression with intercept and large C\n",
    "        x:     matrix on which the model was fit\n",
    "    This function uses asymtptics for maximum likelihood estimates.\n",
    "    \"\"\"\n",
    "    p = model.predict_proba(x)\n",
    "    n = len(p)\n",
    "    m = len(model.coef_[0]) + 1\n",
    "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "    ans = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
    "    vcov = np.linalg.inv(np.matrix(ans))\n",
    "    se = np.sqrt(np.diag(vcov))\n",
    "    t =  coefs/se  \n",
    "    p = (1 - norm.cdf(abs(t))) * 2\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_coef_p_val(model, x):\n",
    "    print(\"                                       Name      Coeff     Exp(Coeff)   p-val   signif\")\n",
    "\n",
    "    for n,c,p in zip(['intercept']+list(x.columns), [model.intercept_[0]] + list(model.coef_[0]), logit_pvalue(model, x)):\n",
    "        s = ''\n",
    "        if p<0.0001: s = '***'\n",
    "        elif p<0.001: s = '**'\n",
    "        elif p<0.05: s = '*'\n",
    "        elif p<0.1: s = '.'\n",
    "        print(\"%45s   %3.3f      %3.3f    %10.3E  %s\" %(n,c,np.exp(c),p,s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_coef_p_val(clf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "Hyperparameters to tune\n",
    "* kernel\n",
    "* degree (se polynomial kernel)\n",
    "* gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch cross validation for hyperparameter tuning\n",
    "\n",
    "parameters = {'kernel' : ('linear', 'poly', 'rbf', 'sigmoid'), \n",
    "              'degree' : [1,2], \n",
    "              'gamma' : ('scale', 'auto'), \n",
    "              'max_iter' :  [-1, 500]} \n",
    "\n",
    "\n",
    "clf = SVC(class_weight = 'balanced', random_state = 0)\n",
    "\n",
    "\n",
    "best_params = print_clf_gridsearchcv_scores(clf, parameters, X_train, y_train, 'SVM')\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clf with best params\n",
    "\n",
    "clf = SVC(class_weight = 'balanced', random_state = 0, **best_params ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final fit and evaluate model\n",
    "\n",
    "clf = clf_fit_print_test_scores(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Hyperparameters to tune\n",
    "\n",
    "* n_estimators\n",
    "* min_samples_split\n",
    "* min_samples_leaf\n",
    "* max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch cross validation for hyperparameter tuning\n",
    "\n",
    "parameters = {'n_estimators' : (45,100,120), #100\n",
    "'max_depth' : (3,4,7), #4\n",
    "'min_samples_split' : (2,3), #2\n",
    "'min_samples_leaf' : (1,2), #1\n",
    "'max_features' : ('log2', None)} #log\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state = 0)\n",
    "\n",
    "\n",
    "best_params = print_clf_gridsearchcv_scores(clf, parameters, X_train, y_train, 'GB')\n",
    "print(best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clf with best params\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state = 0, **best_params ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final fit and evaluate model\n",
    "\n",
    "clf = clf_fit_print_test_scores(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf\n",
    "X_plot = X_train\n",
    "c = 'cornflowerblue' # 'orange' 'cornflowerblue'\n",
    "\n",
    "# get importance\n",
    "importance = clf.feature_importances_\n",
    "df_importance = pd.DataFrame(X_plot.columns,  columns = ['feature'])\n",
    "df_importance['importance'] = importance\n",
    "\n",
    "# sorty by importance\n",
    "df_importance = df_importance.sort_values(by = 'importance', ascending = False)\n",
    "df_importance = df_importance[df_importance.importance > 0]\n",
    "\n",
    "# plot 15 most important features nonn zero\n",
    "plt.bar(df_importance.feature[:15], df_importance.importance[:15] , color = c)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Gradient Boosting')\n",
    "plt.show()\n",
    "\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Hyperparameters to tune\n",
    "\n",
    "* n_estimators\n",
    "* min_samples_split\n",
    "* min_samples_leaf\n",
    "* max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch cross validation for hyperparameter tuning\n",
    "\n",
    "parameters = {'n_estimators' : (30,45,100), #45\n",
    "'max_depth' : (5,7,9), #7\n",
    "'min_samples_split' : (3,4), #4\n",
    "'min_samples_leaf' : (1,2), #2\n",
    "'max_features' : ('log2', None)} #log2\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "\n",
    "best_params = print_clf_gridsearchcv_scores(clf, parameters, X_train, y_train, 'RF')\n",
    "print(best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clf with best params\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, **best_params ).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final fit and evaluate model\n",
    "\n",
    "clf = clf_fit_print_test_scores(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "from joblib import dump, load\n",
    "dump(clf, 'RF_model.joblib')\n",
    "\n",
    "clf = load('RF_model.joblib')\n",
    "\n",
    "clf.predict_proba(X_test)\n",
    "\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%0.3f accuracy\" % (accuracy_score(predictions, y_test)))\n",
    "print(\"%0.3f balanced\" % (balanced_accuracy_score(predictions, y_test)))  \n",
    "print(\"%0.3f precision\" % (precision_score(predictions, y_test)))\n",
    "print(\"%0.3f recall\" % (recall_score(predictions, y_test)))\n",
    "print(\"%0.3f f1 score\" % (f1_score(predictions, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10, 6), dpi=80)\n",
    "\n",
    "clf = clf\n",
    "X_plot = X_train\n",
    "c = 'cornflowerblue' # 'orange' 'cornflowerblue'\n",
    "\n",
    "# get importance\n",
    "importance = clf.feature_importances_\n",
    "df_importance = pd.DataFrame(X_plot.columns,  columns = ['feature'])\n",
    "df_importance['importance'] = importance\n",
    "\n",
    "# sorty by importance\n",
    "df_importance = df_importance.sort_values(by = 'importance', ascending = False)\n",
    "df_importance = df_importance[df_importance.importance > 0]\n",
    "\n",
    "# plot 15 most important features nonn zero\n",
    "plt.bar(df_importance.feature[:20], df_importance.importance[:20], color = c)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Random Forest')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5), dpi=80)\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "'''y_pred_proba = clf.predict_proba(X_test)[::,0]\n",
    "fpr, tpr, _ = roc_curve(1*(y_test == 0),  y_pred_proba)\n",
    "auc = roc_auc_score(1*(y_test == 0), y_pred_proba)'''\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "#plt.legend(loc=4)\n",
    "plt.title(\"3-years survival/mortality prediction ROC curve \\nAUC=\"+str(round(auc, 3)))\n",
    "plt.xlabel(\"FP rate\")\n",
    "plt.ylabel(\"TP rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch cross validation for hyperparameter tuning\n",
    "\n",
    "parameters = {'n_neighbors' : (3,4,5,6,7,8,9), \n",
    "              'weights' : ('uniform', 'distance')} \n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "best_params = print_clf_gridsearchcv_scores(clf, parameters, X_train, y_train, 'KNN')\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clf with best params\n",
    "\n",
    "clf = KNeighborsClassifier(**best_params ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final fit and evaluate model\n",
    "\n",
    "clf = clf_fit_print_test_scores(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utils functions for keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions for test/validation phase\n",
    "\n",
    "def print_dnn_cv_scores(model, X_train, y_train, name, num_epochs, batch):\n",
    "\n",
    "    # define folds\n",
    "\n",
    "    cv_folds = 5 # 10\n",
    "    kf = StratifiedKFold(n_splits = cv_folds, random_state = 42, shuffle = True) \n",
    "    fold_var = 1\n",
    "\n",
    "    # prepare structure to save results for every fold\n",
    "    VALIDATION_LOSS = []\n",
    "    VALIDATION_ACCURACY = []\n",
    "    VALIDATION_RECALL = []\n",
    "    VALIDATION_PRECISION = []\n",
    "    VALIDATION_BALANCED_ACCURACY = []\n",
    "    history_save_loss = np.zeros(num_epochs)\n",
    "    history_save_val_loss = np.zeros(num_epochs)\n",
    "    history_save_acc = np.zeros(num_epochs)\n",
    "    history_save_val_acc = np.zeros(num_epochs)\n",
    "\n",
    "    # cycle model fitting and evaluation on folds\n",
    "    for train_index, val_index in kf.split(np.zeros(X_train.shape[0]),y_train):\n",
    "\n",
    "        # split data according to fold\n",
    "        training_data = X_train.iloc[train_index]\n",
    "        validation_data = X_train.iloc[val_index]\n",
    "        training_label = y_train.iloc[train_index]\n",
    "        validation_label = y_train.iloc[val_index]\n",
    "\n",
    "        # compile model defining loss, optimizer and metrics\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", \"Precision\",\"Recall\"]) \n",
    "\n",
    "        # reshape input as tensors\n",
    "        training_data = training_data.to_numpy()\n",
    "        validation_data = validation_data.to_numpy()\n",
    "\n",
    "        training_data = training_data.reshape(training_data.shape[0], training_data.shape[1], 1)\n",
    "        validation_data = validation_data.reshape(validation_data.shape[0], validation_data.shape[1], 1)\n",
    "\n",
    "        # fit the model\n",
    "        history = model.fit(training_data, training_label,\n",
    "                epochs=num_epochs, batch_size=batch,\n",
    "                validation_data=(validation_data, validation_label),\n",
    "                verbose = 0)\n",
    "\n",
    "        # evaluate the model\n",
    "        results = model.evaluate(validation_data, validation_label, verbose = 0)\n",
    "        results = dict(zip(model.metrics_names,results))\n",
    "\n",
    "        # save results\n",
    "        VALIDATION_LOSS.append(results['loss'])\n",
    "        VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "        VALIDATION_RECALL.append(results['recall'])\n",
    "        VALIDATION_PRECISION.append(results['precision'])\n",
    "\n",
    "        history_save_loss += np.array(history.history['loss'])\n",
    "        history_save_val_loss += np.array(history.history['val_loss'])\n",
    "        history_save_acc += np.array(history.history['accuracy'])\n",
    "        history_save_val_acc += np.array(history.history['val_accuracy'])\n",
    "\n",
    "        predictions = (model.predict(validation_data) > 0.5).astype(\"int32\")   # recall that the output layer activation is sigmoid\n",
    "        VALIDATION_BALANCED_ACCURACY.append(balanced_accuracy_score(predictions, validation_label))\n",
    "\n",
    "        tf.keras.backend.clear_session() # avoid memory consumption over time when creating many models in a loop\n",
    "\n",
    "        fold_var += 1\n",
    "\n",
    "    # compute mean results and print\n",
    "    VALIDATION_LOSS = np.array(VALIDATION_LOSS)\n",
    "    VALIDATION_ACCURACY = np.array(VALIDATION_ACCURACY)\n",
    "    VALIDATION_RECALL = np.array(VALIDATION_RECALL)\n",
    "    VALIDATION_PRECISION = np.array(VALIDATION_PRECISION)\n",
    "    VALIDATION_F1 = 2*(VALIDATION_PRECISION*VALIDATION_RECALL)/(VALIDATION_PRECISION+VALIDATION_RECALL)\n",
    "    VALIDATION_BALANCED_ACCURACY = np.array(VALIDATION_BALANCED_ACCURACY)\n",
    "\n",
    "    print('-------- CV MEAN SCORES (k = '+str(cv_folds) +') ' + name + ' --------')\n",
    "    print(\"%0.3f loss with a standard deviation of %0.3f\" % (VALIDATION_LOSS.mean(), VALIDATION_LOSS.std()))\n",
    "    print(\"%0.3f accuracy with a standard deviation of %0.3f\" % (VALIDATION_ACCURACY.mean(), VALIDATION_ACCURACY.std()))\n",
    "    print(\"%0.3f precision with a standard deviation of %0.3f\" % (VALIDATION_PRECISION.mean(), VALIDATION_PRECISION.std()))\n",
    "    print(\"%0.3f recall with a standard deviation of %0.3f\" % (VALIDATION_RECALL.mean(), VALIDATION_RECALL.std()))\n",
    "    print(\"%0.3f f1 score with a standard deviation of %0.3f\" % (VALIDATION_F1.mean(),VALIDATION_F1.std()))\n",
    "    print(\"%0.3f balanced accuracy score with a standard deviation of %0.3f\" % (VALIDATION_BALANCED_ACCURACY.mean(),VALIDATION_BALANCED_ACCURACY.std()))\n",
    "\n",
    "    return (history.epoch, history_save_loss/cv_folds, history_save_val_loss/cv_folds, history_save_acc/cv_folds, history_save_val_acc/cv_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_cv_mean_learning_curves_plot(history):\n",
    "\n",
    "    # print loss\n",
    "    epochs, loss, val_loss, acc, val_acc =  history \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(epochs, loss)\n",
    "    plt.plot(epochs, val_loss)\n",
    "    plt.title('Mean Loss')\n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.xlabel('epochs')\n",
    "    plt.show()\n",
    "\n",
    "    # print accuracy\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(epochs, acc)\n",
    "    plt.plot(epochs, val_acc)\n",
    "    plt.title('Mean Accuracy') \n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.xlabel('epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions for evaluation phase\n",
    "\n",
    "def dnn_fit_learning_curves_plot(model, X_train, y_train, X_test, y_test, num_epochs, batch):\n",
    "\n",
    "    # reshape input as tensors\n",
    "    X_train = X_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    # re compile and fit model on all training data\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    # plot loss and accuracy learning curves\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history.epoch, history.history['loss'])\n",
    "    plt.plot(history.epoch, history.history['val_loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.xlabel('epochs')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history.epoch, history.history['accuracy'])\n",
    "    plt.plot(history.epoch, history.history['val_accuracy'])\n",
    "    plt.title('Accuracy') \n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.xlabel('epochs')\n",
    "    plt.show() \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_print_test_scores(model, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # reshape input as tensors\n",
    "    X_train = X_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    # test model \n",
    "    print('-------------------- TRAIN SCORES -------------------')\n",
    "    predictions = (model.predict(X_train) > 0.5).astype(\"int32\")  # recall that the output layer activation is sigmoid\n",
    "    print(classification_report(predictions, y_train))\n",
    "    print('balanced accuracy ', balanced_accuracy_score(predictions, y_train))\n",
    "\n",
    "    print('-------------------- TEST SCORES ---------------------')\n",
    "    predictions = (model.predict(X_test) > 0.5).astype(\"int32\")   # recall that the output layer activation is sigmoid\n",
    "    print(classification_report(predictions, y_test))\n",
    "    print('balanced accuracy ', balanced_accuracy_score(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model\n",
    "* first layer input (size = num features)\n",
    "* 2 dense hidden layers (ReLU activation)\n",
    "* last layer activation=\"sigmoid\" and loss=\"binary_crossentropy\" since binary classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Dense Neural Network model\n",
    "\n",
    "def create_dnn(num_features, dense_units = 16, reg_term = 0.01):\n",
    "    # layers\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape = (num_features)))\n",
    "    model.add(keras.layers.Dense(dense_units, activation=\"relu\", kernel_regularizer = tf.keras.regularizers.l2(reg_term)))\n",
    "    model.add(keras.layers.Dense(dense_units/2, activation=\"relu\", kernel_regularizer = tf.keras.regularizers.l2(reg_term)))\n",
    "    #model.add(keras.layers.Dense(dense_units/4, activation=\"relu\", kernel_regularizer = tf.keras.regularizers.l2(reg_term)))\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\")) # output layer for binary classification\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model hyperparameters (only manually test some combinations with CV to avoid long computation time)\n",
    "\n",
    "dense_units_s = 16 #16, 8, 32     # number of units for the dense layers\n",
    "num_epochs_s = 100 #50, 100 #300, 250   # number of training epochs\n",
    "reg_term_s = 0.001 #0.01       # regularization factor\n",
    "batch_s = 256 #128             # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNNs = create_dnn(num_features = X_train.shape[1], dense_units = dense_units_s, reg_term = reg_term_s)\n",
    "h = print_dnn_cv_scores(DNNs, X_train, y_train, 'DNN', num_epochs = num_epochs_s, batch = batch_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "dnn_cv_mean_learning_curves_plot(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and save model\n",
    "\n",
    "DNNs = create_dnn(num_features = X_train.shape[1], dense_units = dense_units_s, reg_term = reg_term_s)\n",
    "DNNs_fit = dnn_fit_learning_curves_plot(DNNs, X_train, y_train, X_test, y_test, num_epochs = 100, batch = batch_s)\n",
    "DNNs_fit.save(\"DNNs trained.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and evaluate model\n",
    "DNNs_fit = keras.models.load_model(mainPath+'DNNs trained_new.h5')\n",
    "dnn_print_test_scores(DNNs_fit, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot roc curve and AUC\n",
    "\n",
    "plt.figure(figsize=(7, 5), dpi=80)\n",
    "\n",
    "y_pred_proba = DNNs_fit.predict(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "#plt.legend(loc=4)\n",
    "plt.title(\"3-years survival/mortality prediction ROC curve \\nAUC=\"+str(round(auc, 3)))\n",
    "plt.xlabel(\"FP rate\")\n",
    "plt.ylabel(\"TP rate\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
